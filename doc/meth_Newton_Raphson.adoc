== Newton - Raphson's method
:icons: font
:stem: latexmath

=== Unidimensional problem

Consider the problem non-linear of one-variable

[stem]
++++
f(x) = 0 \quad (1)
++++

We will construct a good approximation towards the root stem:[x_r] of the equation (1)
by a proceeding called iterative.
We start with a  good estimate stem:[x_0] (initial guess) of stem:[x_r]. From stem:[x_0], we produce an improved
estimate stem:[x_1] by using the linear approximation (tangent line)

[stem]
++++
f(x) \simeq f(x_0) + f'(x_0)(x - x_0)
++++

this approximation must satisfies equation (1), which mean

[stem]
++++
0 \simeq f(x_0) + f'(x_0)(x - x_0)
++++

the new improved estimate stem:[x_1] is therefore given by

[stem]
++++
x_1 = x_0 - \frac{f(x_0)}{f'(x_0)}
++++

the next estimate stem:[x_2] is obtained from stem:[x_1] in exactly the same way as
stem:[x_1] was obtained from stem:[x_0]

[stem]
++++
x_2 = x_1 - \frac{f(x_1)}{f'(x_1)}
++++

and, more generally

[stem]
++++
x_{n + 1} = x_n - \frac{f(x_n)}{f'(x_n)}
++++

.Illustration
image: image/n1.png[]


This method is guaranteed to converge if the initial guess stem:[x_0] is close enough, but it is hard to
make a clear statement about what we mean by ‘close enough’ because this is highly
problem specific. A sketch of the graph of f(x) can help us decide on an appropriate
initial guess x0 for a particular problem.



===  Matrix problem

Consider the system of n equations and n variables

[stem]
++++
\begin{cases}
f_1 (x_1, \cdots, x_n) = 0
\\
\vdots
\\
f_n (x_1, \cdots, x_n) = 0
\end{cases}

++++

The system can be written in a single expression using vectors, i.e.,

[stem]
++++
f(x) = 0
++++

where the vector x contains the independent variables, and the vector f contains the
functions fi(x):

[stem]
++++
x =
\begin{bmatrix}
x_1
\\
x_2
\\
\vdots
\\
x_n
\end{bmatrix} ,

f(x) =
\begin{bmatrix}
f_1(x_1, x_2, \cdots, x_n)
\\
f_2(x_1, x_2, \cdots, x_n)
\\
\vdots
\\
f_n(x_1, x_2, \cdots, x_n)
\end{bmatrix} =

\begin{bmatrix}
f_1(x)
\\
f_2(x)
\\
\vdots
\\
f_n(x)
\end{bmatrix}

++++


A Newton-Raphson method for solving the system of linear equations requires the
evaluation of a matrix, known as the Jacobian of the system, which is defined as:

[stem]
++++
J = \frac{\partial (f_1, f_2, \cdots , f_n)}{\partial(x_1, x_2, \cdots, x_n)}
=
\begin{bmatrix}
\frac{\partial f_1}{\partial x_1} &  \frac{\partial f_1}{\partial x_2} & \cdots & \frac{\partial f_1}{\partial x_n}
\\
\frac{\partial f_2}{\partial x_1} &  \frac{\partial f_2}{\partial x_2} & \cdots & \frac{\partial f_2}{\partial x_n}
\\
\vdots & \vdots & \ddots & \vdots
\\
\frac{\partial f_n}{\partial x_1} &  \frac{\partial f_n}{\partial x_2} & \cdots & \frac{\partial f_n}{\partial x_n}

\end{bmatrix}

= \left[ \frac{\partial f_i}{\partial x_j} \right]_{n \times n}

++++

If stem:[x = x_0] (a vector) represents the first guess for the solution, we can discretize the expression

[stem]
++++
f_i \left( {x}^{(1)} \right) - f_i \left( {x}^{(0)} \right) = \sum_{i=1}^{n} \left( \frac{\partial f_i}{\partial x_j} \right) (x_j^{(1)} - x_j^{(0)})
\quad i = 1, \cdots, n
++++

We want stem:[f_i \left( {x}^{(1)} \right) = 0], so the successive approximations to the solution ate obtained from

[stem]
++++
J(x^{(k)}) \delta x^{(k)} = - R^{(k)}
++++

where stem:[R^{(k)}] is the vector of residual in the k-th iteration

[stem]
++++
R^{(k)} = f\left( x^{(k)} \right)
++++

and stem:[J(x^{(k)})] is the Jacobian matrix in the k-th iteration

[stem]
++++
\left(J(x^{(k)}) \right)_{ij} = \frac{\partial f_i}{\partial x^{(k)}_j}
++++

and stem:[\delta x^{(k)} = x^{(k+1)} - x^{(k)} ]

So we obtain the new estimation

[stem]
++++
x^{(k+1)} = x^{(k)} + \delta x^{(k)}
++++

A convergence criterion for the solution of a system of non-linear equation could be, for
example, that the maximum of the absolute values of the functions stem:[fi(x)] is smaller than a
certain tolerance stem:[\epsilon], i.e.,

[stem]
++++
max _{i}| f_i(x)  |< \epsilon
++++

Newton's method is an extremely powerful technique—in general the convergence is
 quadratic: as the method converges on the root, the difference between the root
 and the approximation is squared (the number of accurate digits roughly doubles)
  at each step. However, there are some difficulties with the method.

* Difficulty in calculating derivative of a function

Newton's method requires that the derivative be calculated directly. An analytical
 expression for the derivative may not be easily obtainable and could be expensive
 to evaluate. In these situations, it may be appropriate to approximate the
 derivative by using the slope of a line through two nearby points on the function.

* Failure of the method to converge to the root

A large error in the initial estimate can contribute to non-convergence of the algorithm.

* Non-quadratic convergence

In some cases the iterates converge but do not converge as quickly as promised.
In these cases simpler methods converge just as quickly as Newton's method.
